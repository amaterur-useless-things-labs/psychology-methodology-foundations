# Lecture 21: Replication Crisis and Its Consequences

## 1. Introduction

In early 2010s, psychology was shaken by replication crisis — it was discovered that many psychological studies do not reproduce when repeated. This caused crisis of trust in psychology and forced reconsideration of research methodology.

In this lecture, we will examine replication crisis: what it is, causes of crisis (p-hacking, HARKing, publication bias), methodological lessons (preregistration, open science), and what changed after crisis.

**Lecture goal:** Understand causes of replication crisis, methodological problems it revealed, and changes in psychology methodology after crisis.

---

## 2. Problem Statement

### The Problem of Replication in Psychology

**Why is this important?**

In early 2010s, it was discovered that many psychological studies do not reproduce:
- **"Open Science Collaboration" Project (2015):** attempt to reproduce 100 studies
- **Result:** only 36% successfully reproduced
- **Crisis of Trust:** trust in psychology undermined

Why did this happen? What methodological problems did it reveal? How did this change psychology?

**The Problem of Reproducibility:**

**What is Replication?**
- **Direct Replication:** repeating study with same methods
- **Conceptual Replication:** testing same hypothesis with different methods

**Why is Replication Important?**
- Confirms reliability of results
- Tests truth of discoveries
- Foundation of scientific progress

**The Problem in Psychology:**

In psychology, problem of reproducibility is especially acute:
- Many studies do not reproduce
- Crisis of trust in psychology
- Changes in methodology needed

**These problems do not have simple answers.** Different approaches solve them differently. It is important to understand causes of crisis and methods to overcome it.

---

## 3. Main Thoughts and Ideas

### 3.1. What is Replication Crisis?

**History of Crisis:**

**2011-2012: First Signals**
- Articles about problem of reproducibility in psychology
- Discussion of methodological problems

**2015: Open Science Collaboration Project**
- Attempt to reproduce 100 studies from top journals
- **Result:** only 36% successfully reproduced
- **Effects:** average effect size decreased from 0.57 to 0.21
- **Shock:** shock to psychological community

**2016-2018: Development of Crisis**
- More replication projects
- Discussion of methodological problems
- Development of open science

**Scale of Problem:**

1. **Low Reproducibility:**
   - Many studies do not reproduce
   - Reproducibility estimates: 36-50%
   - Depends on area (social psychology worse, cognitive better)

2. **Problem of Effects:**
   - Effect sizes are inflated
   - Many effects are weak or absent
   - Problem of statistical power

3. **Crisis of Trust:**
   - Trust in psychology undermined
   - Questions about scientificity of psychology
   - Need for changes

**Examples of Problematic Studies:**

1. **Power Posing Effect:**
   - Study: power poses increase confidence
   - Problem: does not reproduce
   - Causes: small sample, p-hacking

2. **Replication Effect:**
   - Many classic effects do not reproduce
   - Example: Stroop effect, false memory effect
   - But some reproduce

**Literature:**
- **Open Science Collaboration, "Estimating the Reproducibility of Psychological Science"** (2015, English)
  - Classic study of replication crisis

### 3.2. Causes of Crisis

**1. P-Hacking (Distortion of P-Value)**

**What is P-Hacking?**

P-hacking is practice of manipulating data or analyses to obtain statistically significant results (p < 0.05):
- Removing outliers
- Changing inclusion criteria
- Testing multiple hypotheses
- Stopping data collection upon reaching significance

**Examples of P-Hacking:**

1. **Removing Outliers:**
   - Removing data that "interfere" with significance
   - Problem: arbitrariness of criteria

2. **Changing Criteria:**
   - Changing participant inclusion criteria
   - Problem: fitting to result

3. **Testing Multiple Hypotheses:**
   - Testing many hypotheses, publishing only significant ones
   - Problem: multiple comparisons

4. **Stopping Data Collection:**
   - Stopping upon reaching significance
   - Problem: stopping effect

**Consequences of P-Hacking:**

- **False Discoveries:** significant results that do not reproduce
- **Inflated Effects:** effect sizes are inflated
- **Crisis of Trust:** trust in results undermined

**Literature:**
- **Simonsohn, Nelson, Simmons, "P-Curve: A Key to the File-Drawer"** (2014, English)
  - On problem of p-hacking

**2. HARKing (Hypothesizing After Results are Known)**

**What is HARKing?**

HARKing is formulating hypotheses after obtaining results:
- Study is conducted
- Results are obtained
- Hypothesis corresponding to results is formulated
- Published as hypothesis testing

**Problem of HARKing:**

- **False Testing:** hypothesis is not tested, but fitted to result
- **Impossibility of Falsification:** cannot falsify hypothesis formulated after results
- **Distortion of Science:** science becomes confirming, not testing

**Examples of HARKing:**

1. **Discovering Effect:**
   - Unexpected effect is discovered
   - Hypothesis formulated post hoc
   - Published as hypothesis testing

2. **Fitting to Theory:**
   - Results interpreted within theory
   - Hypothesis corresponding to interpretation is formulated
   - Published as testing

**Consequences of HARKing:**

- **False Theories:** theories based on random findings
- **Theory Crisis:** theories are not tested, but fitted
- **Loss of Predictive Power:** theories do not predict, but explain after fact

**Literature:**
- **Norbert Kerr, "HARKing: Hypothesizing After the Results are Known"** (1998, English)
  - On problem of HARKing

**3. Publication Bias**

**What is Publication Bias?**

Publication bias is tendency to publish only significant results:
- Significant results published more often
- Non-significant results not published
- False impression of effects created

**Causes of Publication Bias:**

1. **Journal Preferences:**
   - Journals prefer significant results
   - Non-significant results perceived as "failure"

2. **Researcher Preferences:**
   - Researchers prefer to publish significant results
   - Non-significant results "not interesting"

3. **Career Motives:**
   - Publishing significant results important for career
   - Pressure to publish

**Consequences of Publication Bias:**

- **Inflated Effects:** only strong effects published
- **File-Drawer:** unpublished studies accumulate
- **Distortion of Literature:** literature does not reflect reality

**The File-Drawer Problem:**

- Many studies not published
- Estimated that 50-90% of studies not published
- Creates false impression of effects

**Literature:**
- **Robert Rosenthal, "The File-Drawer Problem"** (1979, English)
  - On problem of publication bias

**4. Other Causes**

**Low Statistical Power:**
- Small samples
- Insufficient power to detect effects
- Problem of false negatives

**Multiple Comparisons:**
- Testing many hypotheses
- Problem of multiple comparisons
- False discoveries

**Problem of Sampling:**
- Narrow samples (WEIRD: Western, Educated, Industrialized, Rich, Democratic)
- Problem of generalization
- Cultural differences

**Literature:**
- **Jacob Cohen, "Statistical Power Analysis for the Behavioral Sciences"** (1988, English, Russian trans. 2015)
  - On problem of statistical power

### 3.3. Methodological Lessons: Solutions

**1. Preregistration**

**What is Preregistration?**

Preregistration is registering hypotheses, methods, analyses before data collection:
- Formulating hypotheses in advance
- Describing methods in advance
- Describing analysis plans in advance
- Publishing regardless of results

**Pros of Preregistration:**

1. **Preventing HARKing:**
   - Hypotheses formulated in advance
   - Cannot fit hypotheses to results

2. **Preventing P-Hacking:**
   - Analysis methods determined in advance
   - Cannot manipulate analysis

3. **Transparency:**
   - Everything visible in advance
   - Can compare plans and results

**Problems of Preregistration:**

1. **Flexibility:**
   - Research requires flexibility
   - Cannot determine everything in advance

2. **Exploratory Questions:**
   - Not all studies are hypothetico-deductive
   - Exploratory studies difficult to preregister

**Literature:**
- **Brian Nosek, "Preregistration for Reproducibility"** (2015, English)
  - On preregistration

**2. Open Science**

**What is Open Science?**

Open Science is movement toward openness of scientific data and processes:
- **Open Data:** open data
- **Open Materials:** open materials
- **Open Code:** open code
- **Open Access:** open access

**Pros of Open Science:**

1. **Verifiability:**
   - Data and materials available
   - Can verify results

2. **Reproducibility:**
   - Can reproduce studies
   - Check of reliability

3. **Collaboration:**
   - Data can be used by others
   - Acceleration of progress

**Problems of Open Science:**

1. **Confidentiality:**
   - Some data confidential
   - Ethical limitations

2. **Time:**
   - Preparing data requires time
   - Additional work

3. **Resistance:**
   - Some researchers resist
   - Cultural changes

**Literature:**
- **Brian Nosek, "Promoting an Open Research Culture"** (2015, English)
  - On open science

**3. Registered Reports**

**What are Registered Reports?**

Registered Reports is publication format where peer review happens before data collection:
- Submission of hypotheses, methods, analysis plans
- Peer review
- Conditional acceptance
- Data collection
- Publication regardless of results

**Pros of Registered Reports:**
- Preventing publication bias
- Preventing HARKing, p-hacking
- Publication regardless of results

**Problems of Registered Reports:**
- Requires time
- Not suitable for all types of studies

**Literature:**
- **Chris Chambers, "Registered Reports"** (2013, English)
  - On registered reports

**4. Other Solutions**

**Increasing Statistical Power:**
- Increasing sample sizes
- Collaborations for larger samples
- Problem of resources

**Bayesian Approach:**
- Alternative to frequentist approach
- More flexible analysis
- Problem of subjectivity

**Replication Studies:**
- Encouraging replications
- Publishing replications
- Changing culture

**Literature:**
- **John Kruschke, "Bayesian Data Analysis"** (2011, English)
  - On Bayesian approach

### 3.4. What Changed After Crisis?

**Changes in Methodology:**

1. **Preregistration:**
   - Became more widespread
   - Many journals require preregistration
   - Cultural change

2. **Open Science:**
   - More open data, materials
   - Changing norms
   - Transparency

3. **Registered Reports:**
   - New publication format
   - Spreading
   - Cultural change

4. **Replications:**
   - More replications
   - Encouraging replications
   - Cultural change

**Changes in Culture:**

1. **Critical Approach:**
   - More critical attitude toward results
   - Skepticism
   - Verification

2. **Collaboration:**
   - More collaborations
   - Joint work
   - Data sharing

3. **Education:**
   - Teaching methodology
   - Understanding problems
   - Changing practice

**Problems:**

1. **Resistance:**
   - Some researchers resist changes
   - Cultural inertia
   - Time

2. **Resources:**
   - Changes require resources
   - Time, money
   - Problem of access

3. **Uncertainty:**
   - Not all changes tested
   - Need research on changes
   - Evolution

**Literature:**
- **Brian Nosek, "Changes in Psychology After Crisis"** (2018, English)
  - On changes after crisis

---

## 4. Conclusions

**Key Ideas of the Lecture:**

1. **Replication Crisis:**
   - Discovery of low reproducibility of studies
   - Only 36% successfully reproduced
   - Crisis of trust in psychology

2. **Causes of Crisis:**
   - **P-Hacking:** manipulating data for significance
   - **HARKing:** formulating hypotheses after results
   - **Publication Bias:** publishing only significant results
   - Low statistical power, multiple comparisons

3. **Solutions:**
   - **Preregistration:** registering hypotheses, methods in advance
   - **Open Science:** openness of data, materials
   - **Registered Reports:** peer review before data collection
   - Increasing statistical power

4. **Changes After Crisis:**
   - Changes in methodology
   - Changes in culture
   - More critical approach, collaboration

5. **Problems:**
   - Resistance to changes
   - Problem of resources
   - Uncertainty

**Practical Conclusions:**

1. **Understanding Problems:** Understanding causes of crisis helps avoid problematic practices

2. **Using Solutions:** Using preregistration, open science improves quality of research

3. **Critical Approach:** Critical approach to research results is important

4. **Changing Culture:** Changing culture of science is important for overcoming crisis

---

## 5. Problems

**Open Questions:**

1. **Scale of Problem:**
   - What is real reproducibility of psychology?
   - Does reproducibility differ in different areas?
   - Is there progress?

2. **Effectiveness of Solutions:**
   - Do preregistration, open science really help?
   - How to measure effectiveness?
   - Which solutions are better?

3. **Balance:**
   - How to balance rigor and flexibility?
   - Are requirements too strict?
   - How to ensure innovation?

4. **Cultural Changes:**
   - How to change culture of science?
   - How long will it take?
   - What are obstacles?

5. **Future:**
   - How will psychology develop?
   - What changes are expected?
   - Is there progress?

**Practical Problems:**

1. **Implementing Solutions:**
   - How to implement preregistration, open science?
   - Requires time, resources
   - How to ensure access?

2. **Training:**
   - How to train methodology?
   - Need changes in education
   - How to ensure competence?

3. **Resistance:**
   - How to overcome resistance?
   - Need changes in culture
   - How to ensure support?

**These problems do not have simple answers.** Different approaches solve them differently. It is important to understand causes of crisis and methods to overcome it.

---

## 6. Where to Look for Further Study

### Main Literature

**Classical Works:**

1. **Open Science Collaboration, "Estimating the Reproducibility of Psychological Science"** (2015, English)
   - Classic study of replication crisis

2. **Brian Nosek, "Promoting an Open Research Culture"** (2015, English)
   - On open science

3. **Brian Nosek, "Preregistration for Reproducibility"** (2015, English)
   - On preregistration

### On Problems

4. **Simonsohn, Nelson, Simmons, "P-Curve: A Key to the File-Drawer"** (2014, English)
   - On problem of p-hacking

5. **Norbert Kerr, "HARKing: Hypothesizing After the Results are Known"** (1998, English)
   - On problem of HARKing

6. **Robert Rosenthal, "The File-Drawer Problem"** (1979, English)
   - On problem of publication bias

### On Solutions

7. **Chris Chambers, "Registered Reports"** (2013, English)
   - On registered reports

8. **John Kruschke, "Bayesian Data Analysis"** (2011, English)
   - On Bayesian approach

### On Statistical Power

9. **Jacob Cohen, "Statistical Power Analysis for the Behavioral Sciences"** (1988, English, Russian trans. 2015)
   - On problem of statistical power

### Online Resources

**Open Science:**
- Center for Open Science (cos.io)
- Open Science Framework (osf.io)

**Preregistration:**
- AsPredicted (aspredicted.org)
- Open Science Framework (osf.io/prereg/)

### Practical Assignments

1. Analyze problematic study:
   - Are there signs of p-hacking?
   - Are there signs of HARKing?
   - What problems are there?

2. Create preregistration for study:
   - Formulate hypotheses
   - Describe methods
   - Describe analysis plans

3. Study changes after crisis:
   - What changes occurred?
   - What problems remain?
   - Is there progress?

**Recommendation:** Start by reading Open Science Collaboration's work "Estimating the Reproducibility of Psychological Science" — this is a classic study of crisis. Then study solutions (Nosek on open science and preregistration) — they show how to overcome crisis.

